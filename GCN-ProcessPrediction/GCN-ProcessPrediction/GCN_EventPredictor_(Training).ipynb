{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ishwarvenugopal/GCN-ProcessPrediction/blob/master/GCN_EventPredictor_(Training).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xW0wmkGQYSNJ"
   },
   "source": [
    "# Importing necessary packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JoDejA1v8ju1",
    "outputId": "57eef44c-0ec5-40af-8000-d9c30f95b094",
    "ExecuteTime": {
     "end_time": "2024-07-01T08:38:13.541787800Z",
     "start_time": "2024-07-01T08:38:13.295788400Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install pm4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V9rHPnPB6TAe",
    "ExecuteTime": {
     "end_time": "2024-07-01T08:38:13.546786200Z",
     "start_time": "2024-07-01T08:38:13.328788100Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from numpy import vstack\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "import torch\n",
    "\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.algo.discovery.dfg import algorithm as dfg_algorithm\n",
    "from pm4py.visualization.dfg import visualizer as dfg_vis_fact\n",
    "import math\n",
    "\n",
    "def glorot(tensor):\n",
    "    if tensor is not None:\n",
    "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "\n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)\n",
    "\n",
    "import bisect\n",
    "import warnings\n",
    "from torch._utils import _accumulate\n",
    "from torch import randperm, default_generator\n",
    "\n",
    "class Subset(Dataset):\n",
    "    r\"\"\"\n",
    "    Subset of a dataset at specified indices.\n",
    "\n",
    "    Arguments:\n",
    "        dataset (Dataset): The whole Dataset\n",
    "        indices (sequence): Indices in the whole set selected for subset\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "        \n",
    "def random_split(dataset, lengths, generator=default_generator):\n",
    "    r\"\"\"\n",
    "    Randomly split a dataset into non-overlapping new datasets of given lengths.\n",
    "    Optionally fix the generator for reproducible results, e.g.:\n",
    "\n",
    "    >>> random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    Arguments:\n",
    "        dataset (Dataset): Dataset to be split\n",
    "        lengths (sequence): lengths of splits to be produced\n",
    "        generator (Generator): Generator used for the random permutation.\n",
    "    \"\"\"\n",
    "    if sum(lengths) != len(dataset):\n",
    "        raise ValueError(\"Sum of input lengths does not equal the length of the input dataset!\")\n",
    "\n",
    "    indices = randperm(sum(lengths), generator=generator).tolist()\n",
    "    return [Subset(dataset, indices[offset - length : offset]) for offset, length in zip(_accumulate(lengths), lengths)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rkd6Re9YYcX"
   },
   "source": [
    "# Setting the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wbo3vcPS6ald",
    "ExecuteTime": {
     "end_time": "2024-07-01T08:38:13.572787700Z",
     "start_time": "2024-07-01T08:38:13.348787100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helpdesk dataset\n",
    "dataset_name = \"bpi13_problems\"\n",
    "f = 0\n",
    "path = \"raw_dir/\"+ dataset_name + \"/\" + dataset_name + \"_kfoldcv_\" + str(f) + \"/\" + dataset_name + \"_all.csv\"\n",
    "# save_folder = 'python_files/Results/helpdesk'\n",
    "num_nodes = 9\n",
    "\n",
    "# # BPI dataset\n",
    "\n",
    "# path = 'Data/bpi_12_w.csv'\n",
    "# save_folder = 'python_files/Results/bpi'\n",
    "# dataset = 'bpi'\n",
    "# num_nodes = 6 \n",
    "\n",
    "num_features = 4\n",
    "showProcessGraph = False\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = f'cuda:{1}'\n",
    "# device = 'cuda'\n",
    "num_epochs = 100\n",
    "seed_value = 42\n",
    "# lr_value = 1e-05\n",
    "\n",
    "weighted_adjacency = False\n",
    "binary_adjacency = True\n",
    "laplacian_matrix = True\n",
    "variant = 'laplacianOnBinary' # Choose from ['weighted','binary','laplacianOnWeighted','laplacianOnBinary']\n",
    "\n",
    "num_runs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dlHB6xqR8VOl"
   },
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6BZ98xW4fSY",
    "ExecuteTime": {
     "end_time": "2024-07-01T08:38:13.604785600Z",
     "start_time": "2024-07-01T08:38:13.370789600Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_features (df,total_activities,num_features):\n",
    "  lastcase = ''\n",
    "  firstLine = True\n",
    "  numlines = 0\n",
    "  casestarttime = None\n",
    "  lasteventtime = None\n",
    "  features = []\n",
    "\n",
    "  for i,row in df.iterrows():\n",
    "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\")\n",
    "    if row[0]!=lastcase:\n",
    "        casestarttime = t\n",
    "        lasteventtime = t\n",
    "        lastcase = row[0]\n",
    "        numlines+=1\n",
    "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\n",
    "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\n",
    "    midnight = datetime.fromtimestamp(time.mktime(t)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    timesincemidnight = datetime.fromtimestamp(time.mktime(t))-midnight\n",
    "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\n",
    "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\n",
    "    timediff3 = timesincemidnight.seconds #this leaves only time even occured after midnight\n",
    "    timediff4 = datetime.fromtimestamp(time.mktime(t)).weekday() #day of the week\n",
    "    lasteventtime = t\n",
    "    firstLine = False\n",
    "    feature_list = [timediff,timediff2,timediff3,timediff4]\n",
    "    features.append(feature_list)\n",
    "\n",
    "  df['Feature Vector'] = features\n",
    "  \n",
    "  firstLine = True\n",
    "  NN_features =[]\n",
    "\n",
    "  for i,row in df.iterrows():\n",
    "    if firstLine:\n",
    "      features = np.zeros((total_activities,num_features))\n",
    "      features[row[1] - 1] = row[3]\n",
    "      firstLine = False\n",
    "    else:\n",
    "      if (row[3][0] == 0):\n",
    "        features = np.zeros((total_activities,num_features))\n",
    "        features[row[1] - 1] = row[3]\n",
    "      else:\n",
    "        features = np.copy(prev_row_features)\n",
    "        features[row[1] - 1] = row[3]\n",
    "    prev_row_features = features\n",
    "    NN_features.append(features)  \n",
    "  \n",
    "  return NN_features\n",
    "\n",
    "def generate_labels(df,total_activities):\n",
    "  next_activity = []\n",
    "  next_timestamp = []\n",
    "\n",
    "  for i,row in df.iterrows():\n",
    "    if (i != 0):\n",
    "      if (row[3][0]==0):\n",
    "        next_activity.append(total_activities)\n",
    "      else:\n",
    "        next_activity.append(row[1]-1)\n",
    "  next_activity.append(total_activities)\n",
    "  for i,row in df.iterrows():\n",
    "    if (i != 0):\n",
    "      if (row[3][0]==0):\n",
    "        next_timestamp.append(0)\n",
    "      else:\n",
    "        next_timestamp.append(row[3][0])\n",
    "  next_timestamp.append(0)\n",
    "\n",
    "  return next_activity,next_timestamp\n",
    "\n",
    "class EventLogData(Dataset):\n",
    "  def __init__ (self, input, output):\n",
    "    self.X = input\n",
    "    self.y = output\n",
    "    self.y = self.y.to(torch.float32)\n",
    "    self.y = self.y.reshape((len(self.y),1))\n",
    "\n",
    "  #get the number of rows in the dataset\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  #get a row at a particular index in the dataset\n",
    "  def __getitem__ (self,idx):\n",
    "    return [self.X[idx],self.y[idx]]\n",
    "  \n",
    "  # get the indices for the train and test rows\n",
    "  def get_splits(self):\n",
    "    train_idx = np.load(\"raw_dir/\"+ dataset_name + \"/\" + dataset_name + \"_kfoldcv_\" + str(f) + \"/\" + \"train\" + '_' + \"index\" + \".npy\", allow_pickle=True)\n",
    "    valid_idx = np.load(\"raw_dir/\"+ dataset_name + \"/\" + dataset_name + \"_kfoldcv_\" + str(f) + \"/\" + \"valid\" + '_' + \"index\" + \".npy\", allow_pickle=True)\n",
    "    test_idx = np.load(\"raw_dir/\"+ dataset_name + \"/\" + dataset_name + \"_kfoldcv_\" + str(f) + \"/\" + \"test\" + '_' + \"index\" + \".npy\", allow_pickle=True)\n",
    "    train = Subset(self, train_idx)\n",
    "    valid = Subset(self, valid_idx)\n",
    "    test = Subset(self, test_idx)\n",
    "    return train, valid, test\n",
    "\n",
    "\n",
    "def prepare_data_for_Predictor(NN_features,label):\n",
    "  dataset = EventLogData(NN_features,label)\n",
    "  train, valid, test = dataset.get_splits()\n",
    "  print(train)\n",
    "  train_dl = DataLoader(train, batch_size=1, shuffle = True)\n",
    "  valid_dl = DataLoader(valid, batch_size=1, shuffle = False)\n",
    "  test_dl = DataLoader(test, batch_size = 1, shuffle = False)\n",
    "  return train_dl, valid_dl, test_dl\n",
    "\n",
    "\n",
    "def generate_input_and_labels (path):\n",
    "  df = pd.read_csv(path, sep=',', header=0, index_col=False)\n",
    "  total_unique_activities = num_nodes\n",
    "  NN_features = generate_features(df,total_unique_activities,num_features)\n",
    "  next_activity, next_timestamp = generate_labels(df,total_unique_activities)\n",
    "  NN_features = torch.Tensor(NN_features).to(torch.float32)\n",
    "  next_activity = torch.Tensor(next_activity).to(torch.float32)\n",
    "  next_timestamp = torch.Tensor(next_timestamp).to(torch.float32)\n",
    " \n",
    "  train_dl, valid_dl, test_dl = prepare_data_for_Predictor(NN_features, next_activity)\n",
    "  \n",
    "  return train_dl,valid_dl,test_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Upqo-IbWYiCG"
   },
   "source": [
    "# Getting Adjacency Matrix from Process Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-cisiZP9_i3",
    "ExecuteTime": {
     "end_time": "2024-07-01T08:38:13.643792900Z",
     "start_time": "2024-07-01T08:38:13.419789100Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_process_graph (path):\n",
    "  data = pd.read_csv(path)\n",
    "  num_nodes = data['ActivityID'].nunique() # 9 for helpdesk.csv\n",
    "  cols = ['case:concept:name','concept:name','time:timestamp']\n",
    "  data.columns = cols \n",
    "  data['time:timestamp'] = pd.to_datetime(data['time:timestamp'])\n",
    "  data['concept:name'] = data['concept:name'].astype(str)\n",
    "  log = log_converter.apply(data, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "  dfg = dfg_algorithm.apply(log)\n",
    "  if showProcessGraph:\n",
    "    visualize_process_graph(dfg,log)\n",
    "  max = 0\n",
    "  min = 0\n",
    "  adj = np.zeros((num_nodes,num_nodes))\n",
    "  for k,v in dfg.items():\n",
    "    for i in range(num_nodes):\n",
    "      if(k[0] == str(i+1)):\n",
    "        for j in range(num_nodes):\n",
    "          if (k[1] == str(j+1)):\n",
    "            adj[i][j] = v\n",
    "            if (v > max): max=v\n",
    "            if (v< min): min=v\n",
    "\n",
    "  # print(\"Raw weighted adjacency matrix: {}\".format(adj))\n",
    "  \n",
    "  if binary_adjacency:\n",
    "    for i in range(num_nodes):\n",
    "      for j in range(num_nodes):\n",
    "        if (adj[i][j]!=0):\n",
    "          adj[i][j]=1\n",
    "    # print(\"Binary adjacency matrix: {}\".format(adj))\n",
    "  \n",
    "  D = np.array(np.sum(adj, axis=1))\n",
    "  D = np.matrix(np.diag(D))\n",
    "  # print(\"Degree matrix: {}\".format(D))\n",
    "  \n",
    "  adj = np.matrix(adj)\n",
    "\n",
    "  if laplacian_matrix:\n",
    "    adj = D - adj # Laplacian Transform \n",
    "    # print(\"Laplacian matrix: {}\".format(adj))\n",
    "\n",
    "  # adj = (D**-1)*adj\n",
    "  adj = fractional_matrix_power(D, -0.5)*adj*fractional_matrix_power(D, -0.5)\n",
    "  adj = torch.Tensor(adj).to(torch.float)\n",
    "  \n",
    "  # print(\"Symmetrically normalised Adjacency matrix: {}\".format(adj))\n",
    "  \n",
    "  return adj\n",
    "\n",
    "def visualize_process_graph (dfg,log):\n",
    "  dfg_gv = dfg_vis_fact.apply(dfg, log, parameters={dfg_vis_fact.Variants.FREQUENCY.value.Parameters.FORMAT: \"jpeg\"})\n",
    "  dfg_vis_fact.view(dfg_gv)\n",
    "  dfg_vis_fact.save(dfg_gv,\"dfg.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztVjtvxqYnb3"
   },
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CW3XlEZ591GP",
    "ExecuteTime": {
     "end_time": "2024-07-01T08:38:13.667791900Z",
     "start_time": "2024-07-01T08:38:13.445796400Z"
    }
   },
   "outputs": [],
   "source": [
    "class GCNConv(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, num_features, out_channels):\n",
    "        super(GCNConv, self).__init__()\n",
    "\n",
    "        self.in_channels = num_features\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(num_features, out_channels))\n",
    "        self.bias = Parameter(torch.Tensor(num_nodes))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = adj@x@self.weight\n",
    "        x = torch.flatten(x)\n",
    "        x = x + self.bias\n",
    "        return x\n",
    "\n",
    "class EventPredictor(torch.nn.Module):\n",
    "    def __init__(self,num_nodes, num_features = 4):\n",
    "        super(EventPredictor, self).__init__()\n",
    "\n",
    "        self.layer1 = GCNConv(num_nodes, num_features, out_channels=1)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(num_nodes,256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256,num_nodes+1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.layer1(x,adj)\n",
    "        x = self.layer2(x)\n",
    "        # x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PdGDSmmyYtg_"
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T08:38:13.668789800Z",
     "start_time": "2024-07-01T08:38:13.474798500Z"
    }
   },
   "outputs": [],
   "source": [
    "def multiclass_roc_auc_score(y_test, y_pred, average):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return roc_auc_score(y_test, y_pred, average=average)\n",
    "\n",
    "\n",
    "def multiclass_pr_auc_score(y_test, y_pred, average):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return average_precision_score(y_test, y_pred, average=average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UIatIuLiFnwK",
    "ExecuteTime": {
     "end_time": "2024-07-01T08:38:25.256627700Z",
     "start_time": "2024-07-01T08:38:13.510791200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1, Learning Rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15540\\2965152617.py:113: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  NN_features = torch.Tensor(NN_features).to(torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Subset object at 0x0000029D66606F40>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 20\u001B[0m\n\u001B[0;32m     16\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlr_value)\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# print(\"************* Event Predictor ***************\")\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# print(\"Train size: {}, Validation size:{}, Test size: {}\".format(len(train_dl.dataset),len(valid_dl.dataset),len(test_dl.dataset)))\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# print(model)\u001B[39;00m\n\u001B[1;32m---> 20\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m adj \u001B[38;5;241m=\u001B[39m adj\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     22\u001B[0m epochs_plt \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tuduipytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:987\u001B[0m, in \u001B[0;36mModule.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    983\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    984\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m    985\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[1;32m--> 987\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tuduipytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:639\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    637\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[0;32m    638\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 639\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    641\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    642\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    643\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    644\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    649\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    650\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tuduipytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:662\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    658\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[0;32m    659\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[0;32m    660\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[0;32m    661\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 662\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    663\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[0;32m    664\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tuduipytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:985\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m    982\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[0;32m    983\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    984\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m--> 985\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "lr_value = 1e-04\n",
    "# for lr_run in range(3):\n",
    "#     if lr_run == 0:\n",
    "#         lr_value = 1e-03\n",
    "#     elif lr_run == 1:\n",
    "#         lr_value = 1e-04\n",
    "#     elif lr_run == 2:\n",
    "#         lr_value = 1e-05\n",
    "run = 0\n",
    "for run in range(num_runs):\n",
    "    print(\"Run: {}, Learning Rate: {}\".format(run + 1, lr_value))\n",
    "    model = EventPredictor(num_nodes, num_features)\n",
    "    train_dl, valid_dl, test_dl = generate_input_and_labels(path)\n",
    "    adj = generate_process_graph(path)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr_value)\n",
    "    # print(\"************* Event Predictor ***************\")\n",
    "    # print(\"Train size: {}, Validation size:{}, Test size: {}\".format(len(train_dl.dataset),len(valid_dl.dataset),len(test_dl.dataset)))\n",
    "    # print(model)\n",
    "    model = model.to(device)\n",
    "    adj = adj.to(device)\n",
    "    epochs_plt = []\n",
    "    acc_plt = []\n",
    "    loss_plt = []\n",
    "    valid_loss_plt = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        num_train = 0\n",
    "        training_loss = 0\n",
    "        predictions, actuals = list(), list()\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()  # Clearing the gradients\n",
    "\n",
    "            yhat = model(inputs[0], adj)\n",
    "\n",
    "            loss = criterion(yhat.reshape((1, -1)), targets[0].to(torch.long))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            training_loss += loss.item()\n",
    "\n",
    "            yhat = yhat.to('cpu')\n",
    "            yhat = torch.argmax(yhat)\n",
    "            actual = targets.to('cpu')\n",
    "            actual = actual[0]\n",
    "            predictions.append(yhat)\n",
    "            actuals.append(actual)\n",
    "            num_train += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            num_valid = 0\n",
    "            validation_loss = 0\n",
    "            for i, (inputs, targets) in enumerate(valid_dl):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                yhat_valid = model(inputs[0], adj)\n",
    "                loss_valid = criterion(yhat_valid.reshape((1, -1)), targets[0].to(torch.long))\n",
    "                validation_loss += loss_valid.item()\n",
    "                num_valid += 1\n",
    "\n",
    "        acc = accuracy_score(actuals, predictions)\n",
    "        avg_training_loss = training_loss / num_train\n",
    "        avg_validation_loss = validation_loss / num_valid\n",
    "\n",
    "        if (epoch == 0):\n",
    "            best_loss = avg_validation_loss\n",
    "            torch.save(model.state_dict(), dataset_name + \"_\" + str(f) + 'EventPredictor_parameters_gcn.pt')\n",
    "\n",
    "        if (avg_validation_loss < best_loss):\n",
    "            torch.save(model.state_dict(), dataset_name + \"_\" + str(f) + 'EventPredictor_parameters_gcn.pt')\n",
    "            best_loss = avg_validation_loss\n",
    "\n",
    "        print(\"Epoch: {}, Loss: {}, Accuracy: {}, Validation loss : {}\".format(epoch, avg_training_loss, acc,\n",
    "                                                                               avg_validation_loss))\n",
    "        epochs_plt.append(epoch + 1)\n",
    "        acc_plt.append(acc)\n",
    "        loss_plt.append(avg_training_loss)\n",
    "        valid_loss_plt.append(avg_validation_loss)\n",
    "\n",
    "    model.load_state_dict(torch.load(dataset_name + \"_\" + str(f) + 'EventPredictor_parameters_gcn.pt'))\n",
    "    model.eval()\n",
    "    total_accuracy = 0\n",
    "    result_path = \"result/\" + dataset_name\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    outfile2 = open(result_path + \"/\" + dataset_name + \"_\" + \".txt\", 'a')\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in enumerate(valid_dl):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            yhat_valid = model(inputs[0], adj)\n",
    "            yhat_valid = yhat_valid.to('cpu')\n",
    "            yhat_valid = torch.argmax(yhat_valid)\n",
    "            actual = targets.to('cpu')\n",
    "            actual = actual[0]\n",
    "            all_preds.append(yhat_valid)\n",
    "            all_labels.append(actual)\n",
    "            # precision, recall, fscore, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
    "            auc_score_macro = multiclass_roc_auc_score(all_labels, all_preds, average=\"macro\")\n",
    "            prauc_score_macro = multiclass_pr_auc_score(all_labels, all_preds, average=\"macro\")\n",
    "            # accuracy = total_accuracy / data_length\n",
    "            print(classification_report(all_labels, all_preds, digits=3))\n",
    "            outfile2.write(classification_report(all_labels, all_preds, digits=3))\n",
    "            outfile2.write('\\nAUC: ' + str(auc_score_macro))\n",
    "            outfile2.write('\\nPRAUC: ' + str(prauc_score_macro))\n",
    "            outfile2.write('\\n')\n",
    "\n",
    "            outfile2.flush()\n",
    "            outfile2.close()\n",
    "    # torch.save(model.state_dict(),\n",
    "    #            '{}/EventPredictor_parameters_gcn_{}_{}_{}_run{}.pt'.format(save_folder, dataset, variant, lr_value,\n",
    "    #                                                                    run))\n",
    "    # filepath = '{}/Accuracy_gcn_{}_{}_{}_run{}.txt'.format(save_folder, dataset, variant, lr_value, run)\n",
    "    #\n",
    "    # with open(filepath, 'w') as file:\n",
    "    #     for item in zip(epochs_plt, acc_plt, loss_plt, valid_loss_plt):\n",
    "    #         file.write(\"{}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6hDg-61DHa_X",
    "ExecuteTime": {
     "end_time": "2024-07-01T08:38:25.259635900Z",
     "start_time": "2024-07-01T08:38:25.258634200Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOSGoMz/8Vch+nco7grvQaQ",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1iKULo9ib0KmoDqWwoC5oO8Xi1XJqqgKq",
   "name": "Copy of GCN_EventPredictor_(Training).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tuduipytorch] *",
   "language": "python",
   "name": "conda-env-tuduipytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
